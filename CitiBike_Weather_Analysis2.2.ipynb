{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8024a86-0ddf-4e18-8e7c-83662a63c443",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "### 1. Downloaded and stored all files for New Yorkâ€™s CitiBike trips in 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e5e2d-4264-4c87-9c62-5233e1d965ee",
   "metadata": {},
   "source": [
    "### 2. Created a repo for the project in GitHub, cloned it to your local machine, and created a new virtual environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04627fa-0dfe-4a70-a683-96ba2693b46f",
   "metadata": {},
   "source": [
    "### 3. Installed the libraries and launched JupyterLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143563e-ed63-4b39-900b-031781bf612a",
   "metadata": {},
   "source": [
    "### 4. Imported all necessary libraries, read the data, and joined it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e7bab4-bb1f-44b7-a419-5b58b7c497ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4e4f35-3c37-48ad-a6e3-04d71608ca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1809961300.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_list = [pd.read_csv(file) for file in all_files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type               started_at  \\\n",
      "0  BFD29218AB271154  electric_bike  2022-01-21 13:13:43.392   \n",
      "1  7C953F2FD7BE1302   classic_bike  2022-01-10 11:30:54.162   \n",
      "2  95893ABD40CED4B8  electric_bike  2022-01-26 10:52:43.096   \n",
      "3  F853B50772137378   classic_bike  2022-01-03 08:35:48.247   \n",
      "4  7590ADF834797B4B   classic_bike  2022-01-22 14:14:23.043   \n",
      "\n",
      "                  ended_at       start_station_name start_station_id  \\\n",
      "0  2022-01-21 13:22:31.463  West End Ave & W 107 St          7650.05   \n",
      "1  2022-01-10 11:41:43.422             4 Ave & 3 St          4028.04   \n",
      "2  2022-01-26 11:06:35.227          1 Ave & E 62 St          6753.08   \n",
      "3  2022-01-03 09:10:50.475          2 Ave & E 96 St          7338.02   \n",
      "4  2022-01-22 14:34:57.474          6 Ave & W 34 St          6364.10   \n",
      "\n",
      "              end_station_name end_station_id  start_lat  start_lng  \\\n",
      "0  Mt Morris Park W & W 120 St        7685.14  40.802117 -73.968181   \n",
      "1      Boerum Pl\\t& Pacific St        4488.09  40.673746 -73.985649   \n",
      "2              5 Ave & E 29 St        6248.06  40.761227 -73.960940   \n",
      "3              5 Ave & E 29 St        6248.06  40.783964 -73.947167   \n",
      "4              5 Ave & E 29 St        6248.06  40.749640 -73.988050   \n",
      "\n",
      "     end_lat    end_lng member_casual  \n",
      "0  40.804038 -73.945925        member  \n",
      "1  40.688489 -73.991160        member  \n",
      "2  40.745168 -73.986831        member  \n",
      "3  40.745168 -73.986831        member  \n",
      "4  40.745168 -73.986831        member  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_path = r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\"\n",
    "\n",
    "# Get all CSV files in the data folder\n",
    "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))  # Correct wildcard usage\n",
    "\n",
    "# Read and concatenate all CSV files into a single DataFrame\n",
    "df_list = [pd.read_csv(file) for file in all_files]\n",
    "bike_data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Display first few rows\n",
    "print(bike_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa94f4ee-1568-4f9b-8974-8c567693f153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202201-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202201-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202202-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202202-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202203-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202203-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202204-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202204-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202204-citibike-tripdata_3.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202205-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202205-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202205-citibike-tripdata_3.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202206-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202206-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202206-citibike-tripdata_3.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202206-citibike-tripdata_4.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202207-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202207-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202207-citibike-tripdata_3.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202207-citibike-tripdata_4.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202208-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202208-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202208-citibike-tripdata_3.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202208-citibike-tripdata_4.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202209-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202209-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202209-citibike-tripdata_3.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202209-citibike-tripdata_4.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202210-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202210-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202210-citibike-tripdata_3.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202211-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202211-citibike-tripdata_2.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202211-citibike-tripdata_3.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202212-citibike-tripdata_1.csv...\n",
      "Processing P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\\202212-citibike-tripdata_2.csv...\n",
      "Merged CSV file saved at: P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\merged_citibike_data_2022.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the path to the data folder\n",
    "data_path = r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\Exercise 2.2\\2022-citibike-tripdata\"\n",
    "\n",
    "# Get all CSV files in the data folder\n",
    "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))  \n",
    "\n",
    "# Define the export path for the merged CSV file\n",
    "export_path = os.path.join(r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\", \"merged_citibike_data_2022.csv\")\n",
    "\n",
    "# Create an empty CSV file with headers from the first file\n",
    "if all_files:\n",
    "    first_chunk = pd.read_csv(all_files[0], encoding=\"utf-8\", low_memory=False, nrows=5)  # Read just a few rows to get columns\n",
    "    first_chunk.to_csv(export_path, index=False, encoding=\"utf-8\")  # Save headers\n",
    "\n",
    "    # Process each file in chunks and append to the CSV\n",
    "    for file in all_files:\n",
    "        print(f\"Processing {file}...\")\n",
    "        chunk_iter = pd.read_csv(file, encoding=\"utf-8\", low_memory=False, chunksize=100000)  # Read in chunks\n",
    "        \n",
    "        for chunk in chunk_iter:\n",
    "            chunk.to_csv(export_path, mode=\"a\", header=False, index=False, encoding=\"utf-8\")  # Append data without headers\n",
    "\n",
    "print(f\"Merged CSV file saved at: {export_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefca53-2299-4f16-905b-75c559d04a6f",
   "metadata": {},
   "source": [
    "### \n",
    "### Reading in Chunks:\n",
    "#### The dataset is read in smaller chunks (chunksize=100000), which prevents memory overload when dealing with large files.\n",
    "### Concatenation Incrementally:\n",
    "#### Rather than reading all files at once and concatenating them in one go, the data is read in chunks and added to a list (df_list). After processing all chunks, the DataFrame is concatenated.\n",
    "### Efficient Export:\n",
    "#### After the data is successfully processed, it's exported to a CSV file (merged_citibike_data.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0cbfa9-cf16-44de-93a4-151cae297d96",
   "metadata": {},
   "source": [
    "### 5. This Code Works (Step by Step)\n",
    "\n",
    "    Imports required libraries (pandas, os, glob).\n",
    "    Defines the folder path where all CSV files are stored.\n",
    "    Uses glob.glob() to find all .csv files in the directory.\n",
    "    Reads each CSV file into a Pandas DataFrame and stores them in a list.\n",
    "    Concatenates all DataFrames into a single dataset.\n",
    "    Displays the first few rows to verify the data.\n",
    "    Saves the merged dataset for easier use in future steps.\n",
    "\n",
    "\n",
    "Automates file importing â€“ No need to load each file manually.  Handles large datasets efficiently â€“ Uses concat() instead of multiple .append() calls.\n",
    " Scalability â€“ Works even if more monthly data files are added in the future.\n",
    " Debugging-friendly â€“ Prints a warning if no files are found.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db621e-9650-4f53-8165-8c8741ae8976",
   "metadata": {},
   "source": [
    "### 6. Registered to NOAAâ€™s web app and request the token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a1137-18c5-4bed-b80a-e3ce9652bbcc",
   "metadata": {},
   "source": [
    "### 7. Obtained weather data from New York LaGuardiaâ€™s weather station for 2022 and exported it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e993d7c6-047e-44b9-aafe-a3797e2e515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\4042111781.py:40: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  weather_data.to_csv('P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\LaGuardia_Weather_2022.csv', index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data exported successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set up your NOAA API key (replace with your actual API key)\n",
    "api_key = 'MxUzcyewRgbAXckkqUsURxNQgwTskPgn'\n",
    "\n",
    "# LaGuardia station identifier (confirm the correct station code)\n",
    "station_id = 'GHCND:USW00014739'\n",
    "\n",
    "# API endpoint for historical data (temperature, etc.)\n",
    "url = f'https://www.ncdc.noaa.gov/cdo-web/api/v2/data'\n",
    "\n",
    "# Set the parameters to query the data for 2022\n",
    "params = {\n",
    "    'datasetid': 'GHCND',  # Global Historical Climatology Network Daily Data\n",
    "    'datatypeid': 'TMAX',  # Max temperature, replace with other datatypes if needed\n",
    "    'stationid': station_id,\n",
    "    'startdate': '2022-01-01',\n",
    "    'enddate': '2022-12-31',\n",
    "    'limit': 1000  # Adjust as needed\n",
    "}\n",
    "\n",
    "# Set headers for the API request\n",
    "headers = {\n",
    "    'token': api_key\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Convert the data to JSON\n",
    "    data = response.json()\n",
    "    \n",
    "    # Convert to DataFrame (make sure you adjust based on the response format)\n",
    "    weather_data = pd.DataFrame(data['results'])\n",
    "    \n",
    "    # Export to CSV\n",
    "    weather_data.to_csv('P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\LaGuardia_Weather_2022.csv', index=False)\n",
    "    print(\"Weather data exported successfully.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve data:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7942e9-9038-448a-bf7f-fcbdd8ff856d",
   "metadata": {},
   "source": [
    "### 8. Merged the weather data with the New York CitiBike data set and exported it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4a7530-8431-422c-8517-4fe2daf12a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\283407897.py:4: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bike_data = pd.read_csv(r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\merged_citibike_data_2022.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CitiBike Data:\n",
      "             ride_id  rideable_type started_at ended_at  \\\n",
      "0  BFD29218AB271154  electric_bike    13:43.4  22:31.5   \n",
      "1  7C953F2FD7BE1302   classic_bike    30:54.2  41:43.4   \n",
      "2  95893ABD40CED4B8  electric_bike    52:43.1  06:35.2   \n",
      "3  F853B50772137378   classic_bike    35:48.2  10:50.5   \n",
      "4  7590ADF834797B4B   classic_bike    14:23.0  34:57.5   \n",
      "\n",
      "        start_station_name start_station_id             end_station_name  \\\n",
      "0  West End Ave & W 107 St          7650.05  Mt Morris Park W & W 120 St   \n",
      "1             4 Ave & 3 St          4028.04      Boerum Pl\\t& Pacific St   \n",
      "2          1 Ave & E 62 St          6753.08              5 Ave & E 29 St   \n",
      "3          2 Ave & E 96 St          7338.02              5 Ave & E 29 St   \n",
      "4          6 Ave & W 34 St           6364.1              5 Ave & E 29 St   \n",
      "\n",
      "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \n",
      "0        7685.14  40.802117 -73.968181  40.804038 -73.945925        member  \n",
      "1        4488.09  40.673746 -73.985649  40.688489 -73.991160        member  \n",
      "2        6248.06  40.761227 -73.960940  40.745168 -73.986831        member  \n",
      "3        6248.06  40.783964 -73.947167  40.745168 -73.986831        member  \n",
      "4        6248.06  40.749640 -73.988050  40.745168 -73.986831        member  \n",
      "Weather Data:\n",
      "                   date datatype            station attributes  value\n",
      "0  2022-01-01T00:00:00     TMAX  GHCND:USW00014739   ,,W,2400    111\n",
      "1  2022-01-02T00:00:00     TMAX  GHCND:USW00014739   ,,W,2400     78\n",
      "2  2022-01-03T00:00:00     TMAX  GHCND:USW00014739   ,,W,2400      6\n",
      "3  2022-01-04T00:00:00     TMAX  GHCND:USW00014739   ,,W,2400     -5\n",
      "4  2022-01-05T00:00:00     TMAX  GHCND:USW00014739   ,,W,2400    106\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CitiBike trip data\n",
    "bike_data = pd.read_csv(r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\merged_citibike_data_2022.csv\")\n",
    "\n",
    "# Load weather data (LaGuardia Airport)\n",
    "weather_data = pd.read_csv(r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\laguardia_weather_2022.csv\")\n",
    "\n",
    "# Display the first few rows of both datasets\n",
    "print(\"CitiBike Data:\\n\", bike_data.head())\n",
    "print(\"Weather Data:\\n\", weather_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aa3b010-9a7b-49ee-8682-eecc33d86bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\448479265.py:5: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in bike_data_chunks:\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\448479265.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"started_at\"] = pd.to_datetime(chunk[\"started_at\"], errors=\"coerce\")\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\448479265.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"started_at\"] = pd.to_datetime(chunk[\"started_at\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type          started_at ended_at  \\\n",
      "0  BFD29218AB271154  electric_bike 2025-01-30 13:43:24  22:31.5   \n",
      "1  7590ADF834797B4B   classic_bike 2025-01-30 14:23:00  34:57.5   \n",
      "2  BFD29218AB271154  electric_bike 2025-01-30 13:43:24  22:31.5   \n",
      "3  7590ADF834797B4B   classic_bike 2025-01-30 14:23:00  34:57.5   \n",
      "4  621225A86D88489F  electric_bike 2025-01-30 08:37:00  26:01.9   \n",
      "\n",
      "        start_station_name start_station_id             end_station_name  \\\n",
      "0  West End Ave & W 107 St          7650.05  Mt Morris Park W & W 120 St   \n",
      "1          6 Ave & W 34 St           6364.1              5 Ave & E 29 St   \n",
      "2  West End Ave & W 107 St          7650.05  Mt Morris Park W & W 120 St   \n",
      "3          6 Ave & W 34 St           6364.1              5 Ave & E 29 St   \n",
      "4          6 Ave & W 34 St           6364.1      Allen St & Rivington St   \n",
      "\n",
      "  end_station_id  start_lat  start_lng    end_lat    end_lng member_casual  \\\n",
      "0        7685.14  40.802117 -73.968181  40.804038 -73.945925        member   \n",
      "1        6248.06  40.749640 -73.988050  40.745168 -73.986831        member   \n",
      "2        7685.14  40.802117 -73.968181  40.804038 -73.945925        member   \n",
      "3        6248.06  40.749640 -73.988050  40.745168 -73.986831        member   \n",
      "4        5414.06  40.749640 -73.988050  40.720196 -73.989978        member   \n",
      "\n",
      "         date datatype station attributes  value  \n",
      "0  2025-01-30      NaN     NaN        NaN    NaN  \n",
      "1  2025-01-30      NaN     NaN        NaN    NaN  \n",
      "2  2025-01-30      NaN     NaN        NaN    NaN  \n",
      "3  2025-01-30      NaN     NaN        NaN    NaN  \n",
      "4  2025-01-30      NaN     NaN        NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1000000  # Adjust based on your system's memory capacity\n",
    "bike_data_chunks = pd.read_csv(r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\merged_citibike_data_2022.csv\", chunksize=chunk_size)\n",
    "\n",
    "merged_data = pd.DataFrame()\n",
    "for chunk in bike_data_chunks:\n",
    "    # Handle invalid date formats\n",
    "    chunk[\"started_at\"] = pd.to_datetime(chunk[\"started_at\"], errors=\"coerce\")\n",
    "    \n",
    "    # Remove rows with invalid dates (if any)\n",
    "    chunk = chunk[chunk[\"started_at\"].notna()]\n",
    "    \n",
    "    chunk[\"date\"] = chunk[\"started_at\"].dt.date\n",
    "    \n",
    "    # Merge with weather data (you can read weather data once before merging)\n",
    "    merged_chunk = pd.merge(chunk, weather_data, on=\"date\", how=\"left\")\n",
    "    merged_data = pd.concat([merged_data, merged_chunk], ignore_index=True)\n",
    "\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5316a347-72e1-4307-93f5-85aae96c7923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1419030348.py:5: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in bike_data_chunks:\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1419030348.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"started_at\"] = pd.to_datetime(chunk[\"started_at\"], errors=\"coerce\")\n",
      "C:\\Users\\sojan\\AppData\\Local\\Temp\\ipykernel_23324\\1419030348.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk[\"started_at\"] = pd.to_datetime(chunk[\"started_at\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data exported successfully to: P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\merged_bike_weather_data.csv\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1000000  # Adjust based on your system's memory capacity\n",
    "bike_data_chunks = pd.read_csv(r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\\merged_citibike_data_2022.csv\", chunksize=chunk_size)\n",
    "\n",
    "merged_data = pd.DataFrame()\n",
    "for chunk in bike_data_chunks:\n",
    "    # Handle invalid date formats\n",
    "    chunk[\"started_at\"] = pd.to_datetime(chunk[\"started_at\"], errors=\"coerce\")\n",
    "    \n",
    "    # Remove rows with invalid dates (if any)\n",
    "    chunk = chunk[chunk[\"started_at\"].notna()]\n",
    "    \n",
    "    chunk[\"date\"] = chunk[\"started_at\"].dt.date\n",
    "    \n",
    "    # Merge with weather data (you can read weather data once before merging)\n",
    "    merged_chunk = pd.merge(chunk, weather_data, on=\"date\", how=\"left\")\n",
    "    merged_data = pd.concat([merged_data, merged_chunk], ignore_index=True)\n",
    "\n",
    "# Define the export path for the merged CSV file\n",
    "export_path = os.path.join(r\"P:\\CarrerFoundy\\Data Visualizations with Python\\Achivement 2\", \"merged_bike_weather_data.csv\")\n",
    "\n",
    "# Export the merged data to a CSV file\n",
    "merged_data.to_csv(export_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Merged data exported successfully to: {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19d0e1-0cdc-495e-be28-a2321c5871cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
